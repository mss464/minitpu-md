# ISA

NOTE: This document is a design contract between the RTL and software subteams.
Any change to encoding or semantics must be coordinated and updated here.

## Scope
This document defines the CornellTPU 64-bit instruction encoding and execution
semantics. It is the single source of truth for instruction layout, mode
values, and per-op behavior.

## Instruction Word Overview
- Fixed-width 64-bit instructions stored in IRAM and executed sequentially.
- Bit numbering uses [63] as MSB and [0] as LSB.
- Common address fields (all modes):
  - ADDR_A  [61:49]
  - ADDR_B  [48:36]
  - ADDR_OUT [35:23]
- Lower bits are mode-specific:
  - VPU: ADDR_CONST [22:10], OPCODE [9:0]
  - Systolic/Vadd: LEN [22:0]

## Mode Field
MODE is encoded in bits [63:62].

| MODE | Meaning              |
|-----:|----------------------|
|   0  | VPU instruction      |
|   1  | Systolic instruction |
|   2  | Vadd (test unit)      |
|   3  | Halt                 |

Invalid mode values are undefined.

## Addressing and Data Model
- All ADDR_* fields are 13-bit word addresses into BRAM (0..8191).
- Each BRAM word is 32 bits (fp32 in the current system).
- ADDR_CONST points at a BRAM word containing a constant (commonly zero).
- IRAM holds up to 256 instructions. Programs must end with HALT.

## Instruction Formats

### VPU (MODE = 0)

The VPU supports both legacy scalar operations and modern SIMD vector operations
through a VPU_TYPE field.

**Common VPU Fields:**

| Field      | Bits    | Description                          |
|------------|---------|--------------------------------------|
| MODE       | [63:62] | Must be 0                            |
| ADDR_A     | [61:49] | Base address of input/load vector    |
| ADDR_B     | [48:36] | Base address of input vector B       |
| ADDR_OUT   | [35:23] | Base address of output/store vector  |
| VPU_TYPE   | [22:20] | VPU instruction class (3 bits)       |
| (variable) | [19:0]  | Type-specific fields                 |

**VPU_TYPE Values:**

| VPU_TYPE | Class      | Description                              |
|---------:|------------|------------------------------------------|
|        0 | SCALAR     | Legacy element-wise operations           |
|        1 | VLOAD      | Load 8 FP32 values to vector register    |
|        2 | VSTORE     | Store 8 FP32 values from vector register |
|        3 | VCOMPUTE   | SIMD vector operations (8 lanes)         |
|      4-7 | RESERVED   | Future extension                         |

---

#### VPU Type 0: SCALAR (Legacy)

**Format:**

| Field      | Bits    | Description                          |
|------------|---------|--------------------------------------|
| MODE       | [63:62] | 0                                    |
| ADDR_A     | [61:49] | Base address of input vector A       |
| ADDR_B     | [48:36] | Base address of input vector B       |
| ADDR_OUT   | [35:23] | Base address of output vector        |
| VPU_TYPE   | [22:20] | 0                                    |
| ADDR_CONST | [19:7]  | Base address of constant element     |
| OPCODE     | [6:0]   | VPU operation selector               |

**Semantics:** Operates on a single element per instruction. To process longer
vectors, software must issue multiple instructions.

**Opcodes:**

| Opcode | Mnemonic        | Operation                   |
|------:|-----------------|-----------------------------|
|     0 | ADD             | Element-wise addition       |
|     1 | SUB             | Element-wise subtraction    |
|     2 | RELU            | Rectified Linear Unit       |
|     3 | MUL             | Element-wise multiplication |
|     4 | RELU_DERIVATIVE | ReLU derivative             |

**ADDR_CONST usage:**
- RELU and RELU_DERIVATIVE use ADDR_CONST (typically pointing at a zero value).
- ADD, SUB, and MUL ignore ADDR_CONST; set it to 0.

---

#### VPU Type 1: VLOAD

Load 8 consecutive FP32 values from BRAM into a vector register.

**Format:**

| Field      | Bits    | Description                          |
|------------|---------|--------------------------------------|
| MODE       | [63:62] | 0                                    |
| ADDR_A     | [61:49] | BRAM start address                   |
| ADDR_B     | [48:36] | 0 (reserved)                         |
| ADDR_OUT   | [35:23] | 0 (reserved)                         |
| VPU_TYPE   | [22:20] | 1                                    |
| RESERVED   | [19:17] | 0                                    |
| VREG_DST   | [16:14] | Destination vector register (0-7)    |
| RESERVED   | [13:0]  | 0                                    |

**Semantics:** `VREG_DST[0:7] = BRAM[ADDR_A + 0:7]`

**Example:**
```
vload v0, 0x100   # Load BRAM[0x100:0x107] → V0[0:7]
```

**Timing:** ~10-15 cycles (8 sequential BRAM reads with latency)

---

#### VPU Type 2: VSTORE

Store 8 FP32 values from a vector register to BRAM.

**Format:**

| Field      | Bits    | Description                          |
|------------|---------|--------------------------------------|
| MODE       | [63:62] | 0                                    |
| ADDR_OUT   | [61:49] | BRAM destination address             |
| ADDR_B     | [48:36] | 0 (reserved)                         |
| ADDR_A     | [35:23] | 0 (reserved)                         |
| VPU_TYPE   | [22:20] | 2                                    |
| RESERVED   | [19:17] | 0                                    |
| VREG_SRC   | [16:14] | Source vector register (0-7)         |
| RESERVED   | [13:0]  | 0                                    |

**Semantics:** `BRAM[ADDR_OUT + 0:7] = VREG_SRC[0:7]`

**Example:**
```
vstore v2, 0x200  # Store V2[0:7] → BRAM[0x200:0x207]
```

**Timing:** ~10-15 cycles (8 sequential BRAM writes)

---

#### VPU Type 3: VCOMPUTE

SIMD arithmetic operations on vector registers (8-lane parallel execution).

**Format:**

| Field      | Bits    | Description                          |
|------------|---------|--------------------------------------|
| MODE       | [63:62] | 0                                    |
| RESERVED   | [61:23] | 0                                    |
| VPU_TYPE   | [22:20] | 3                                    |
| VREG_DST   | [19:17] | Destination register (0-7)           |
| VREG_A     | [16:14] | Source register A (0-7)              |
| VREG_B     | [13:11] | Source register B (0-7)              |
| RESERVED   | [10:7]  | 0                                    |
| OPCODE     | [6:4]   | Vector operation (3 bits)            |
| SCALAR_B   | [3]     | 1 = broadcast VREG_B[0] to all lanes |
| RESERVED   | [2:0]   | 0                                    |

**Semantics (vector-vector, SCALAR_B=0):**
```
for i in 0..7:
    VREG_DST[i] = OP(VREG_A[i], VREG_B[i])
```

**Semantics (vector-scalar, SCALAR_B=1):**
```
scalar = VREG_B[0]  # Broadcast element 0
for i in 0..7:
    VREG_DST[i] = OP(VREG_A[i], scalar)
```

**OPCODE Values:**

| OPCODE | Mnemonic | Operation                |
|-------:|----------|--------------------------|
|      0 | VADD     | result = a + b           |
|      1 | VSUB     | result = a - b           |
|      2 | VMUL     | result = a * b           |
|      3 | VRELU    | result = max(a, 0)       |
|      4 | VMAX     | result = max(a, b)       |
|      5 | VMIN     | result = min(a, b)       |
|    6-7 | RESERVED | -                        |

**Examples:**
```
vadd v2, v0, v1       # V2[i] = V0[i] + V1[i] for i=0..7
vmul v3, v1, v0.s     # V3[i] = V1[i] * V0[0] (scalar broadcast)
vrelu v1, v1          # V1[i] = max(V1[i], 0) (in-place ReLU)
```

**Timing:** 1 cycle (true parallel execution)

---

#### Vector Register File

The SIMD VPU includes 8 vector registers (V0-V7), each holding 8 FP32 elements.

**Register File Specifications:**
- Number of registers: 8 (V0-V7)
- Elements per register: 8
- Element width: 32 bits (FP32)
- Total size per register: 256 bits
- Total register file size: 2048 bits (256 bytes)

**Access Properties:**
- Dual-port read (for binary operations like VADD)
- Single-port write
- Asynchronous read, synchronous write
- Reset initializes all registers to zero

**Usage Pattern:**
1. VLOAD: Load BRAM data into vector registers
2. VCOMPUTE: Perform SIMD operations on registers
3. VSTORE: Write results back to BRAM

---

#### SIMD VPU Example Programs

**Example 1: Vector Addition**
```
vload v0, 0x100        # V0 = A[0:7]
vload v1, 0x200        # V1 = B[0:7]
vadd v2, v0, v1        # V2 = V0 + V1
vstore v2, 0x300       # C[0:7] = V2
```
**Performance:** 4 instructions, ~25 cycles total (vs. 8 scalar VPU instructions @ 64 cycles)

**Example 2: Vector Scaling (Scalar Broadcast)**
```
vload v0, 0x100        # V0 = input[0:7]
vload v1, 0x200        # V1 = [scale, _, _, ...]
vmul v2, v0, v1.s      # V2 = V0 * V1[0] (broadcast)
vstore v2, 0x300       # output[0:7] = V2
```

**Example 3: Fused Multiply-Add (FMA pattern)**
```
vload v0, 0x100        # V0 = x[0:7]
vload v1, 0x200        # V1 = w[0:7]
vmul v2, v0, v1        # V2 = x * w
vload v3, 0x300        # V3 = bias[0:7]
vadd v2, v2, v3        # V2 = x*w + bias
vrelu v2, v2           # V2 = ReLU(x*w + bias)
vstore v2, 0x400       # Store result
```
**Performance:** 7 instructions, ~40 cycles (vs. 24 scalar VPU @ 192 cycles)

**Example 4: In-Place ReLU Activation**
```
vload v0, 0x100        # V0 = activations[0:7]
vrelu v0, v0           # V0 = max(V0, 0) (in-place)
vstore v0, 0x100       # Write back in-place
```

---

#### Performance Comparison: Scalar vs. SIMD VPU

| Operation | Scalar VPU | SIMD VPU (8-lane) | Speedup |
|-----------|------------|-------------------|---------|
| 8-element ADD | 8 instr × 8 cycles = 64 | 1 VADD (~1 cycle) + LOAD/STORE (~25 total) | ~2.5x |
| 16-element MUL | 16 × 8 = 128 cycles | 2 VMUL + LOAD/STORE (~50 total) | ~2.5x |
| Dot product (16) | 16 MUL + 15 ADD = 248 | 2 VMUL + 2 VADD + LOAD/STORE (~55 total) | ~4.5x |

**Note:** Actual speedup depends on LOAD/STORE overhead. For compute-heavy workloads
with register reuse, SIMD VPU approaches 8x speedup for pure compute operations.

### Systolic (MODE = 1)

| Field    | Bits    | Description                               |
|----------|---------|-------------------------------------------|
| MODE     | [63:62] | Must be 1                                 |
| ADDR_A   | [61:49] | Base address of weight matrix W           |
| ADDR_B   | [48:36] | Base address of input matrix X            |
| ADDR_OUT | [35:23] | Base address of output matrix Z           |
| LEN      | [22:0]  | Number of words to process (see note)     |

**Semantics**

The systolic array computes: `Z = X @ W^T` (input times weight-transposed).

Given `matmul W, X, Z`:
- W is stored row-major at ADDR_A (4x4 = 16 words)
- X is stored row-major at ADDR_B (4x4 = 16 words)
- Result Z = X @ W^T is written row-major to ADDR_OUT

This transposed-weight convention matches common neural network usage where
weights are stored in `[out_features, in_features]` layout.

**Notes**
- The current systolic array performs 4x4 matrix multiplication.
- LEN is encoded but currently ignored by hardware. Set LEN = 16 by convention.
- For larger matrices, use software tiling with VPU accumulation (see below).

### Vadd (MODE = 2)

| Field    | Bits    | Description                          |
|----------|---------|--------------------------------------|
| MODE     | [63:62] | Must be 2                            |
| ADDR_A   | [61:49] | Base address of input vector A       |
| ADDR_B   | [48:36] | Base address of input vector B       |
| ADDR_OUT | [35:23] | Base address of output vector        |
| LEN      | [22:0]  | Vector length (number of elements)   |

Vadd is a simple test compute unit used for system bring-up. It performs:
`OUT[i] = A[i] + B[i]` for `i = 0..LEN-1`. LEN must be >= 1.

### Halt (MODE = 3)

HALT marks the end of the instruction stream. All bits other than MODE must be
zero. On HALT, the TPU transitions to IDLE.

## Execution Model
- Instructions are fetched from IRAM and executed in program order.
- No branching or predication; each instruction completes before the next.
- All programs must terminate with a HALT instruction.
- Unused fields must be zero. Invalid MODE or OPCODE values are undefined.

## Encoding Constraints
- MODE is 2 bits.
- ADDR_* fields are 13 bits (0..8191).
- LEN is 23 bits (0..8,388,607) but must not cause BRAM overrun.
- OPCODE is 10 bits; only values 0..4 are currently defined.

## Assembler Mnemonics (Software)
The current assembler uses these mnemonics:
- Systolic: `matmul <addr_w> <addr_x> <addr_out>`
- VPU Scalar: `add`, `sub`, `relu`, `mul`, `relu_derivative`
- VPU SIMD:
  - `vload <vreg> <addr>` - Load BRAM to vector register
  - `vstore <vreg> <addr>` - Store vector register to BRAM
  - `vadd <vdst> <va> <vb> [.s]` - Vector addition (.s = scalar broadcast)
  - `vsub <vdst> <va> <vb> [.s]` - Vector subtraction
  - `vmul <vdst> <va> <vb> [.s]` - Vector multiplication
  - `vrelu <vdst> <vsrc>` - Vector ReLU
  - `vmax <vdst> <va> <vb>` - Vector maximum
  - `vmin <vdst> <va> <vb>` - Vector minimum
- Control: `halt`
- Pseudo: `load <addr> <len> <values>`, `store <addr> <len> <label>`

## Tiled Matrix Multiplication

For matrices larger than 4x4, software tiling decomposes the computation into
multiple 4x4 matmuls with VPU accumulation.

**Algorithm for MxN = MxK @ KxN (tile size t=4):**
```
for i in range(M // t):
    for j in range(N // t):
        for k in range(K // t):
            if k == 0:
                Z[i,j] = X[i,k] @ W[j,k]^T      # matmul
            else:
                temp = X[i,k] @ W[j,k]^T        # matmul to temp
                Z[i,j] += temp                  # VPU add (16 elements)
```

**Instruction count for MxM @ MxM tiled matmul:**
- Tiles: (M/4)^3 matmuls
- Accumulations: (M/4)^2 * ((M/4) - 1) * 16 VPU adds
- Example: 8x8 = 8 matmuls + 64 adds = 73 instructions (including HALT)

**Tile-major memory layout:**
Matrices must be stored in tile-major order for efficient access:
```
[tile(0,0), tile(0,1), ..., tile(1,0), tile(1,1), ...]
```
Each tile is stored row-major within the tile (16 contiguous words for 4x4).

## Hardware Constraints

| Constraint | Value | Notes |
|------------|-------|-------|
| IRAM size | 256 instructions | Programs exceeding this fail at compile time |
| BRAM size | 8192 words | 32KB total (fp32) |
| Tile size | 4x4 | Fixed systolic array dimension |
| Max working matrix | 8x8 | Larger matrices exceed instruction limit |

**Execution model:** All instructions must be in a single batch. The TPU does
not support multiple execution phases within one program load. To process
larger workloads, reload data and instructions between compute() calls.
